# Borderless Table Detection from image
This project implements a borderless table detection system using Hugging Face's Table Transformer model. The system is capable of detecting and extracting tables from images even when no visible borders are separating the rows and columns. It uses deep learning models to detect tables and their structure and visualizes the results by marking the tables, rows, and columns on the original image.

# How to Run
For quickly view and run the notebook from Kaggle: [Link to my Kaggle Notebook](https://www.kaggle.com/code/shakilrana/borderless-table-detection)

1. Clone the Repository (if needed):
   ```bash
   git clone https://github.com/ShakilMahmudShuvo/Borderless-Tables-Detection.git
   ```
2. Ensure the required Python libraries are installed. You can use the requirements.txt or directly install them in your environment:
   (this step is optional though, as the notebook also contains installing the requirements)
   ```
    pip install -r requirements.txt
   ```
3. Open the Jupyter Notebook (`borderless_table_detection.ipynb`), Run each cell in sequence to load the models, process the images, and visualize the results
4. In the notebook, there is a **Inference** section, where you can set the test image path and run the inference pipeline to visualize the result.

## Approach

1. **Image Loading and Preprocessing**  
   The input image is first loaded using the **Pillow (PIL)** library. Since the model expects the image in RGB format, the image is converted from any format (e.g., grayscale, CMYK) to RGB. Additionally, to make image processing more efficient and less computationally expensive, the image is resized to 50% of its original dimensions. Resizing ensures that the processing pipeline operates faster without significant loss in table detection accuracy. This trade-off between speed and accuracy is particularly important when dealing with large datasets or high-resolution images.

   The image is then passed through the **DetrFeatureExtractor** provided by Hugging Face, which converts the image into a tensor format compatible with PyTorch. This extracted feature tensor serves as the input to the table detection and structure recognition models.

2. **Model Selection and Loading**  
   This project leverages two models from Hugging Face’s **Table Transformer** suite, specifically designed for **table detection** and **table structure recognition**. These models are based on **DEtection TRansformers (DETR)**, which combine convolutional neural networks (CNNs) with a transformer architecture to capture both local and global features of an image.

   - **Table Detection Model**:  
     The table detection model (`microsoft/table-transformer-detection`) identifies areas within the image that contain tables. This is essentially an object detection task where tables are treated as objects. The model processes the extracted feature tensors and outputs bounding boxes around the detected tables. These bounding boxes are further refined using confidence thresholds to remove low-confidence detections.

     The model architecture includes a **ResNet backbone** followed by a **transformer encoder-decoder** structure. The **ResNet backbone** captures local features (like edges or corners), while the **transformer layers** model long-range dependencies, which are crucial for understanding table layouts, especially when no borders are present.

   - **Table Structure Recognition Model**:  
     Once the table detection model identifies the tables, the table structure recognition model (`microsoft/table-transformer-structure-recognition`) is employed to detect the internal structure of the detected tables. This model predicts the arrangement of rows, columns, and potentially spanning cells within each table. The model’s output consists of bounding boxes and labels, where each label corresponds to a specific element in the table (row, column, etc.).

     Similar to the detection model, the structure recognition model also uses a **DETR-based architecture** with **multi-head attention** to capture both the spatial layout and the relationships between different table elements. The model can handle both simple and complex table structures, including those with merged or split cells.

3. **Inference Pipeline**  
   The overall inference process follows these steps:
   
   - **Feature Extraction**: The image tensor generated by the `DetrFeatureExtractor` is fed into the selected model (either detection or structure recognition). The transformer layers compute attention maps that help the model focus on relevant parts of the image.
   
   - **Table Detection**: The detection model predicts bounding boxes around tables, treating this as an object detection problem. The bounding boxes are post-processed using a confidence threshold to filter out weak detections, ensuring that only highly probable tables are kept.

   - **Table Structure Detection**: For each detected table, the structure recognition model predicts bounding boxes and class labels for rows, columns, and other structural elements. This is critical for understanding the internal layout of the table, which is crucial when the table is borderless.

   - **Post-processing**: The final step includes mapping the model outputs to interpretable bounding boxes. The post-processing function ensures that the bounding boxes are correctly scaled back to the original image size and are filtered based on the confidence score. This reduces false positives and ensures that only the most relevant table structures are kept.

4. **Result Visualization**  
   The results are visualized by drawing bounding boxes around detected tables, rows, and columns on the original image. Different colors are used to distinguish between these elements. These visualizations provide a clear understanding of how well the models have detected both the table boundaries and internal structures.

   Here is a test image and the inference results from the architecture:
   
   **The original test image**
   ![borderless](https://github.com/user-attachments/assets/de3e108e-33ef-4ac6-a524-918c30353964)
    Detecting table output from inferring the `microsoft/table-transformer-detection` model:
   ![detect_table](https://github.com/user-attachments/assets/1ceb3a0a-c579-4ac5-ad9d-5b3f092c2df4)
    Identifying the borderless rows and column (inferring from the `microsoft/table-transformer-structure-recognition` model)
   ![identify_borderless](https://github.com/user-attachments/assets/9ab4899d-6425-4685-841d-de39ce277b60)


  

  
